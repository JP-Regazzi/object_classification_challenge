{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joao Pedro\\Desktop\\Agenda-Guia\\CS\\3A\\Mention\\2025.1\\Data Challenge\\object_classification_challenge\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from random import random, randrange\n",
    "from torchvision import transforms,models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from copy import deepcopy\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from color_analysis import ColorAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import fashion clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "image_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating images lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_IMAGES_CSV_PATH = os.path.join(\"../../data\",\"DAM\")\n",
    "reference_images = [os.path.join(CLEAN_IMAGES_CSV_PATH, img)for img in os.listdir(CLEAN_IMAGES_CSV_PATH)]\n",
    "\n",
    "TEST_IMAGES_PATH = os.path.join(\"../../data\",\"test_image_headmind\")\n",
    "img_list = [os.path.join(TEST_IMAGES_PATH,i) for i in os.listdir(TEST_IMAGES_PATH)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Answers Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_10680\\4145771647.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.read_csv(\"results17-32.csv\",header=None, sep=\", \")\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_10680\\4145771647.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_arthur = pd.read_csv(\"answer65-80.csv\", header=None, sep=\", \")\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_10680\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_10680\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
      "C:\\Users\\Joao Pedro\\AppData\\Local\\Temp\\ipykernel_10680\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n"
     ]
    }
   ],
   "source": [
    "csv_all = pd.read_csv(\"results17-32.csv\",header=None, sep=\", \")\n",
    "csv_arthur = pd.read_csv(\"answer65-80.csv\", header=None, sep=\", \")\n",
    "csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
    "csv_all.head()\n",
    "csv_all.columns = ['0','1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}\n",
    "for _, row in csv_all.iterrows():\n",
    "    if row['1'] in answers.keys():\n",
    "        answers[row['1']].append(row['0'])\n",
    "    else:\n",
    "        answers[row['1']] = [row['0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings for the referrence images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2766/2766 [03:04<00:00, 14.98it/s]\n"
     ]
    }
   ],
   "source": [
    "reference_features = []\n",
    "for img in tqdm(reference_images):\n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1)\n",
    "        reference_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:14<00:00,  5.62it/s]\n"
     ]
    }
   ],
   "source": [
    "client_features = []\n",
    "for img in tqdm(img_list):\n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1)\n",
    "        client_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reference_features)):\n",
    "    reference_features[i] = reference_features[i].flatten()\n",
    "\n",
    "for i in range(len(client_features)):\n",
    "    client_features[i] = client_features[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_embedding_tensor = torch.stack(client_features).squeeze(1)\n",
    "embedding_tensor = torch.stack(reference_features).squeeze(1)\n",
    "\n",
    "cosine_similarities = torch.mm(client_embedding_tensor, embedding_tensor.t())\n",
    "\n",
    "# closest_indices = torch.argsort(cosine_similarities, dim=1,descending=True)[:,:11]\n",
    "# closest_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# -- HERE: Incorporate the ColorAnalysis “buff” ----\n",
    "####################################################\n",
    "\n",
    "# 1) Load the precomputed color info for reference images from CSV\n",
    "#    The CSV is assumed to be created via save_color_info_to_csv(...).\n",
    "#\n",
    "#    Columns:\n",
    "#    [filename, L1, a1, b1, L2, a2, b2, L3, a3, b3]\n",
    "#\n",
    "color_database_csv = \"./color_database.csv\"  # Adjust if needed\n",
    "df_ref = pd.read_csv(color_database_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform CSV rows into a dictionary:\n",
    "#   reference_color_dict[\"some_image.jpg\"] = np.array([[L1,a1,b1],[L2,a2,b2],[L3,a3,b3]], dtype=np.float32)\n",
    "reference_color_dict = {}\n",
    "for _, row in df_ref.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    colors_flat = [\n",
    "        [row[\"L1\"], row[\"a1\"], row[\"b1\"]],\n",
    "        [row[\"L2\"], row[\"a2\"], row[\"b2\"]],\n",
    "        [row[\"L3\"], row[\"a3\"], row[\"b3\"]]\n",
    "    ]\n",
    "    reference_color_dict[filename] = np.array(colors_flat, dtype=np.float32)\n",
    "\n",
    "# Map the full path of reference_images to their Lab color arrays\n",
    "reference_colors = {}\n",
    "for ref_path in reference_images:\n",
    "    ref_filename = os.path.basename(ref_path)  # just the file name\n",
    "    if ref_filename in reference_color_dict:\n",
    "        reference_colors[ref_path] = reference_color_dict[ref_filename]\n",
    "    else:\n",
    "        reference_colors[ref_path] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading client color data from color_test.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Read client color from csv (if exists) or extract them\n",
    "color_test_csv = \"color_test.csv\"\n",
    "client_colors = {}  # Reset or define anew\n",
    "color_analyzer = ColorAnalysis()\n",
    "\n",
    "if os.path.exists(color_test_csv):\n",
    "    print(\"Loading client color data from color_test.csv ...\")\n",
    "    df_test = pd.read_csv(color_test_csv)\n",
    "    for _, row in df_test.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        color_sets = []\n",
    "        for i in range(1, 4):  # 3 colors\n",
    "            color_sets.append([row[f\"L{i}\"], row[f\"a{i}\"], row[f\"b{i}\"]])\n",
    "        # Convert to numpy array (Lab values)\n",
    "        img_lab = np.array(color_sets, dtype=np.float32)\n",
    "\n",
    "        # Reconstruct full path if needed, or store by filename only\n",
    "        img_path = os.path.join(TEST_IMAGES_PATH, filename)\n",
    "        client_colors[img_path] = img_lab\n",
    "else:\n",
    "    print(\"No color_test.csv found, computing colors on the fly...\")\n",
    "    client_colors = {}\n",
    "    for img_path in tqdm(img_list, desc=\"Extracting Colors for Client Images\"):\n",
    "        colors_lab = color_analyzer.extract_object_colors(img_path, num_colors=3)\n",
    "        client_colors[img_path] = colors_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(color_test_csv):\n",
    "    # AFTER you've computed client_colors, save them to a CSV named \"color_test.csv\"\n",
    "\n",
    "    # Prepare rows for CSV: [filename, L1, a1, b1, L2, a2, b2, L3, a3, b3]\n",
    "    rows = []\n",
    "    for img_path, colors_lab in client_colors.items():\n",
    "        if colors_lab is not None:\n",
    "            flattened = colors_lab.flatten().tolist()\n",
    "            filename = os.path.basename(img_path)\n",
    "            rows.append([filename] + flattened)\n",
    "\n",
    "    # Define columns\n",
    "    columns = [\"filename\"]\n",
    "    for i in range(1, 4):  # num_colors = 3\n",
    "        columns += [f\"L{i}\", f\"a{i}\", f\"b{i}\"]\n",
    "\n",
    "    # Save CSV\n",
    "    df_client = pd.DataFrame(rows, columns=columns)\n",
    "    df_client.to_csv(\"color_test.csv\", index=False)\n",
    "    print(\"Client color info saved to color_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 80, Best boost: 0.03, Accuracy: 93.51%\n"
     ]
    }
   ],
   "source": [
    "### Fine-tuning the color parameters (search to find best ones)\n",
    "import itertools\n",
    "\n",
    "best_threshold = None\n",
    "best_boost = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Some example ranges to iterate over (customize as you wish)\n",
    "threshold_range = [80]\n",
    "boost_range = [0.03, 0.02] \n",
    "\n",
    "for color_threshold, color_boost in itertools.product(threshold_range, boost_range):\n",
    "    # Make a fresh copy of the original cosines\n",
    "    final_sims = cosine_similarities.clone()\n",
    "\n",
    "    num_clients = final_sims.shape[0]\n",
    "    num_refs = final_sims.shape[1]\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        client_img_path = img_list[i]\n",
    "        client_color_set = client_colors[client_img_path]\n",
    "        if client_color_set is None:\n",
    "            continue  # no color info, skip\n",
    "\n",
    "        for j in range(num_refs):\n",
    "            ref_img_path = reference_images[j]\n",
    "            ref_color_set = reference_colors[ref_img_path]\n",
    "            if ref_color_set is None:\n",
    "                continue\n",
    "\n",
    "            # Compute Lab distance\n",
    "            dist = color_analyzer.color_distance(client_color_set, ref_color_set)\n",
    "            if dist < color_threshold:\n",
    "                ratio = dist / color_threshold   # 0..1\n",
    "                bonus = color_boost * (1.0 - ratio)\n",
    "                #bonus = color_boost\n",
    "                final_sims[i, j] += bonus\n",
    "\n",
    "    # Re-rank\n",
    "    closest_indices = torch.argsort(final_sims, dim=1, descending=True)[:, :11]\n",
    "\n",
    "    # Build guesses\n",
    "    guesses = {}\n",
    "    for i, img in enumerate(img_list):\n",
    "        file_name = os.path.basename(img)\n",
    "        img_id = file_name.split(\".\")[0]\n",
    "        guesses[img_id] = [reference_images[idx] for idx in closest_indices[i].tolist()]\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy = 0\n",
    "    nb_guess = 0\n",
    "    for answer_key in answers.keys():\n",
    "        found = 0\n",
    "        if answer_key in guesses.keys():\n",
    "            nb_guess += 1\n",
    "            for value_guess in guesses[answer_key]:\n",
    "                for value_answ in answers[answer_key]:\n",
    "                    if value_answ in value_guess and not found:\n",
    "                        accuracy += 1\n",
    "                        found = 1\n",
    "\n",
    "    final_acc = accuracy / nb_guess * 100 if nb_guess > 0 else 0.0\n",
    "\n",
    "    # Check if it's the best so far\n",
    "    if final_acc > best_accuracy:\n",
    "        best_accuracy = final_acc\n",
    "        best_threshold = color_threshold\n",
    "        best_boost = color_boost\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Best boost: {best_boost}, Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define your thresholds and boost factor\n",
    "color_threshold = 80.0  # distance in Lab; typical from your snippet\n",
    "color_boost     = 0.03  # how much to “push up” similarity if color is close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Combine color info with the existing CLIP similarity\n",
    "#    We'll make a copy of the original cosines so we can store the final\n",
    "#    “color-boosted” similarity in a new tensor.\n",
    "final_sims = cosine_similarities.clone()  # shape [num_clients, num_refs]\n",
    "\n",
    "num_clients = final_sims.shape[0]\n",
    "num_refs    = final_sims.shape[1]\n",
    "\n",
    "for i in range(num_clients):\n",
    "    client_img_path = img_list[i]\n",
    "    client_color_set = client_colors[client_img_path]\n",
    "    if client_color_set is None:\n",
    "        # No color info, skip\n",
    "        continue\n",
    "\n",
    "    for j in range(num_refs):\n",
    "        ref_img_path = reference_images[j]\n",
    "        ref_color_set = reference_colors[ref_img_path]\n",
    "        if ref_color_set is None:\n",
    "            continue\n",
    "\n",
    "        # Compute Lab distance between sets\n",
    "        dist = color_analyzer.color_distance(client_color_set, ref_color_set)\n",
    "        # If below threshold, linearly scale the boost\n",
    "        if dist < color_threshold:\n",
    "            ratio = dist / color_threshold   # 0..1\n",
    "            # Add a partial color_boost, bigger if dist is small\n",
    "            bonus = color_boost * (1.0 - ratio)  \n",
    "            final_sims[i, j] = final_sims[i, j] + bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code back to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Now run the \"closest indices\" selection\n",
    "##############################################\n",
    "closest_indices = torch.argsort(final_sims, dim=1, descending=True)[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = {}\n",
    "\n",
    "for i, img in enumerate(img_list):\n",
    "    file_name = img.split(\"\\\\\")[-1]\n",
    "    img_id = file_name.split(\".\")[0]\n",
    "    guesses[img_id] = [reference_images[ind] for ind in closest_indices[i].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "nb_guess = 0\n",
    "for answer_key in answers.keys():\n",
    "    found = 0\n",
    "    if answer_key in guesses.keys():\n",
    "        nb_guess += 1\n",
    "        for value_guess in guesses[answer_key]:\n",
    "            for value_answ in answers[answer_key]:\n",
    "                if value_answ in value_guess and not found:\n",
    "                    accuracy += 1\n",
    "                    found = 1\n",
    "\n",
    "accuracy/nb_guess*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accuracy counts\n",
    "accuracy_top1 = 0\n",
    "accuracy_top5 = 0\n",
    "accuracy_top10 = 0\n",
    "nb_guess = 0\n",
    "\n",
    "for answer_key in answers.keys():\n",
    "    if answer_key in guesses.keys():\n",
    "        nb_guess += 1\n",
    "        found_top1 = False\n",
    "        found_top5 = False\n",
    "        found_top10 = False\n",
    "\n",
    "        for value_answ in answers[answer_key]:\n",
    "            if any(value_answ in value_guess for value_guess in guesses[answer_key][:1]):\n",
    "                accuracy_top1 += 1\n",
    "                found_top1 = True\n",
    "            if any(value_answ in value_guess for value_guess in guesses[answer_key][:5]):\n",
    "                accuracy_top5 += 1\n",
    "                found_top5 = True\n",
    "            if any(value_answ in value_guess for value_guess in guesses[answer_key][:10]):\n",
    "                accuracy_top10 += 1\n",
    "                found_top10 = True\n",
    "\n",
    "# Compute percentage accuracy\n",
    "accuracy_top1 = accuracy_top1 / nb_guess * 100 if nb_guess > 0 else 0\n",
    "accuracy_top5 = accuracy_top5 / nb_guess * 100 if nb_guess > 0 else 0\n",
    "accuracy_top10 = accuracy_top10 / nb_guess * 100 if nb_guess > 0 else 0\n",
    "print(accuracy_top1, accuracy_top5, accuracy_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../../data/test_image_headmind/IMG_6893.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,img_path in enumerate(img_list):\n",
    "\n",
    "    if i < 10:\n",
    "        continue\n",
    "    if i > 12:\n",
    "        break\n",
    "    file_name = img_path.split(\"\\\\\")[-1]\n",
    "    img_id = file_name.split(\".\")[0]\n",
    "    guess_list = []\n",
    "    for g in guesses[img_id]:\n",
    "        guess_list.append(g)\n",
    "\n",
    "    fig, axs = plt.subplots(2,6,figsize=(20,5))\n",
    "    fig\n",
    "    axs[0,0].imshow(np.swapaxes(mpimg.imread(img_path),0,1))\n",
    "    for j in range(1,12):\n",
    "        axs[int(j/6),int(j%6)].imshow(mpimg.imread(guess_list[j-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
