{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from random import random, randrange\n",
    "from torchvision import transforms,models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import fashion clip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fdc1729f3f45ada35971d3c852de2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\arthu\\.cache\\huggingface\\hub\\models--patrickjohncyh--fashion-clip. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fceef8cc5f4f1aa4503e7a55593d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b88eb677444d65bd146a18c7457b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaf5b88aa1c497995e0f5adb52c13a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412957294f2c4ae8ab618a9ebe62cd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4172c2e5d0db497f9d1fe670e8de4be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb4ca9cd2164d0494cd3a685e8683da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50357dd4f8c84dfcaf471e744b95e7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"patrickjohncyh/fashion-clip\"\n",
    "image_processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating images lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_IMAGES_CSV_PATH = os.path.join(\"data\",\"DAM\")\n",
    "reference_images = [os.path.join(CLEAN_IMAGES_CSV_PATH, img)for img in os.listdir(CLEAN_IMAGES_CSV_PATH)]\n",
    "\n",
    "TEST_IMAGES_PATH = os.path.join(\"data\",\"test_image_headmind\")\n",
    "img_list = [os.path.join(TEST_IMAGES_PATH,i) for i in os.listdir(TEST_IMAGES_PATH)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Answers Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2068\\4145771647.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.read_csv(\"results17-32.csv\",header=None, sep=\", \")\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2068\\4145771647.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_arthur = pd.read_csv(\"answer65-80.csv\", header=None, sep=\", \")\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2068\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2068\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_2068\\4145771647.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n"
     ]
    }
   ],
   "source": [
    "csv_all = pd.read_csv(\"results17-32.csv\",header=None, sep=\", \")\n",
    "csv_arthur = pd.read_csv(\"answer65-80.csv\", header=None, sep=\", \")\n",
    "csv_all = pd.concat([pd.read_csv(\"answers1-16.csv\", header=None, sep= \", \"),csv_all, pd.read_csv(\"results33-48.csv\", header=None, sep=\", \"), pd.read_csv(\"results49-64.csv\", header=None, sep=\", \"), csv_arthur])\n",
    "csv_all.head()\n",
    "csv_all.columns = ['0','1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0538OCALM35R</td>\n",
       "      <td>image-20210928-102713-12d2869d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0505SLOIM989</td>\n",
       "      <td>image-20210928-102718-2474636a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0565ONGEM50P</td>\n",
       "      <td>image-20210928-102721-8eaea48f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M9203UWDIM59E</td>\n",
       "      <td>image-20210928-102725-7e28b44c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M9204UMOAM918</td>\n",
       "      <td>image-20210928-102725-7e28b44c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SOSTELAIR1SZJ86</td>\n",
       "      <td>MicrosoftTeams-image_46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SOSTELAIR1807YB</td>\n",
       "      <td>MicrosoftTeams-image_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DSGTS6UXR10A0</td>\n",
       "      <td>MicrosoftTeams-image_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>S5652CBAAM41G</td>\n",
       "      <td>MicrosoftTeams-image_53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S5652CCEHM900</td>\n",
       "      <td>MicrosoftTeams-image_54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                               1\n",
       "0     M0538OCALM35R  image-20210928-102713-12d2869d\n",
       "1     M0505SLOIM989  image-20210928-102718-2474636a\n",
       "2     M0565ONGEM50P  image-20210928-102721-8eaea48f\n",
       "3     M9203UWDIM59E  image-20210928-102725-7e28b44c\n",
       "4     M9204UMOAM918  image-20210928-102725-7e28b44c\n",
       "..              ...                             ...\n",
       "12  SOSTELAIR1SZJ86         MicrosoftTeams-image_46\n",
       "13  SOSTELAIR1807YB         MicrosoftTeams-image_48\n",
       "14    DSGTS6UXR10A0         MicrosoftTeams-image_50\n",
       "15    S5652CBAAM41G         MicrosoftTeams-image_53\n",
       "16    S5652CCEHM900         MicrosoftTeams-image_54\n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}\n",
    "for _, row in csv_all.iterrows():\n",
    "    if row['1'] in answers.keys():\n",
    "        answers[row['1']].append(row['0'])\n",
    "    else:\n",
    "        answers[row['1']] = [row['0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings for the referrence images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2766/2766 [05:13<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "reference_features = []\n",
    "for img in tqdm(reference_images):\n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1)\n",
    "        reference_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:46<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "client_features = []\n",
    "for img in tqdm(img_list):\n",
    "    image = Image.open(img).convert(\"RGB\")\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1)\n",
    "        client_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reference_features)):\n",
    "    reference_features[i] = reference_features[i].flatten()\n",
    "\n",
    "for i in range(len(client_features)):\n",
    "    client_features[i] = client_features[i].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1666, 1775, 2522, 1719, 2087, 1739, 1696, 1772, 1656, 1657],\n",
       "        [1735, 1716, 1768,  910, 1717, 1779, 2523, 1783, 1816, 1817],\n",
       "        [1696, 2087, 1710, 1778, 1822, 1788, 2522, 1767, 1780, 1766],\n",
       "        [2265, 2281, 2245, 2581, 2269, 2299, 2255, 2247, 2263, 2243],\n",
       "        [2265, 2281, 2288, 2581, 2269, 2243, 2263, 2255, 2235, 2245],\n",
       "        [1426, 1492, 1420, 1376, 1431,  300, 1343, 1185, 1196, 1386],\n",
       "        [2287, 2464, 2299, 2249, 2614, 2227, 2146, 2266, 2582, 2419],\n",
       "        [1817, 1735, 2523, 1768,  910, 1717,  911, 1818, 1688, 1839],\n",
       "        [2651, 2644, 2635, 1828, 1934, 2636, 1827, 1914, 2543, 1915],\n",
       "        [2651, 1934, 1828, 2635, 1827, 1928, 1914, 1929, 1918, 1960],\n",
       "        [ 659,  664,  660, 1057, 1170, 1078, 1172,  663, 2748, 1056],\n",
       "        [1574, 1539, 1591, 1994, 2001, 2542, 1533, 1547, 2002, 2297],\n",
       "        [ 665, 1035, 2744, 2736, 1050,  659, 1036, 1030,  664, 1078],\n",
       "        [2744,  659,  664, 1174, 1078,  660, 1070, 2743, 2736, 1171],\n",
       "        [ 910, 1768, 1716, 1735, 2523, 1885, 1711, 1817,  911, 1839],\n",
       "        [2279, 1818, 2670, 2685, 2656, 1949, 2543, 2684, 2645, 1654],\n",
       "        [1241,  857, 1238, 1461, 1457, 1243,  415, 1412,  414, 1448],\n",
       "        [1272,  701, 1269, 1397, 1281,  648,  488,  382, 2747, 1338],\n",
       "        [1269, 1268, 1281, 1282, 1272, 1279, 1287, 1284, 1245, 1274],\n",
       "        [1251, 1271, 1244, 1266, 1277, 1291, 1245, 1278, 1276, 1279],\n",
       "        [1263, 1266, 1297, 1265, 1291, 1250, 1293, 1245, 1277, 1292],\n",
       "        [1250, 2542,  596,  705, 1376,  836,  608, 1266,  712, 1241],\n",
       "        [1266, 1262, 1245, 1246, 1276, 1289, 1277, 1244, 1265, 1297],\n",
       "        [1233, 1452, 1241, 1213, 1229,  857, 1455, 1461, 1219, 1239],\n",
       "        [1050, 1080, 1069, 1067, 1082,  665,  659,  662, 1065, 1064],\n",
       "        [1069, 1050,  904, 1074,  903, 1511, 1082, 1080, 1076,  665],\n",
       "        [1067, 1168, 1050, 1069, 2736, 1080, 2741,  904, 1079, 1081],\n",
       "        [ 905,  664,  904,  665, 1173, 1074, 1077, 1064, 1172, 1076],\n",
       "        [ 665, 1076,  904, 1074, 1050,  659, 1069, 1077,  906,  664],\n",
       "        [1050, 1079, 1078, 1174, 1069,  665,  659,  664, 1511, 2736],\n",
       "        [1077,  665, 1076, 1069,  904,  664, 1050, 1074, 1511, 1073],\n",
       "        [ 904, 1069,  665, 1074, 1076,  657, 1082, 1077, 1080,  659],\n",
       "        [1075, 1511, 1077, 1078, 2741, 1069, 1072, 1074, 2382, 1064],\n",
       "        [ 664,  665,  659, 1172, 2382,  903, 1050, 2738, 1171,  905],\n",
       "        [1168, 1068, 2744, 1067, 2735, 2736, 2743, 2742, 1512, 1171],\n",
       "        [2248, 2272, 2296, 2663, 2488, 2292, 2245, 2582, 1527, 2271],\n",
       "        [2247, 2245, 2265, 2243, 2263, 2581, 2272, 2291, 2281, 2264],\n",
       "        [1654, 2271, 1838, 2614, 2267, 1602, 1644, 2244, 1609, 1818],\n",
       "        [2282, 2285, 2272, 2271, 2281, 2583, 2243, 2263, 1574, 2581],\n",
       "        [1574, 2001, 1994, 2014, 1985, 2006, 2002, 1999, 1540, 1995],\n",
       "        [2003, 1995, 1540, 1574, 1994, 1531, 2001, 1985, 2006, 1999],\n",
       "        [1933, 1962, 1935, 1969, 1914, 1936, 1932, 1973, 1970, 1967],\n",
       "        [1828, 2635, 1914, 1827, 1970, 2651, 1830, 1832, 1886, 1965],\n",
       "        [2261, 2262, 2259, 2263, 2243, 2575, 2264, 2271, 2247, 2257],\n",
       "        [1735, 2523, 1817, 2076, 1839, 1768, 1691, 1779, 2137,  910],\n",
       "        [2304, 2307, 2318, 2300, 2320, 2321, 2317, 1710, 2308, 1680],\n",
       "        [2311, 2302, 2296, 2293, 2300, 2301, 1735, 2407, 1782, 1817],\n",
       "        [ 917,  927,  918,  928,  922,  923,  916,  954,  934,  978],\n",
       "        [ 928,  927,  934,  914,  920,  925,  950,  981,  917,  982],\n",
       "        [ 917,  918,  922,  928, 1003,  914,  923, 1012,  927, 1004],\n",
       "        [ 836,  233,   93,  231,    1,  603,  232,  714, 2718,  234],\n",
       "        [1766, 1710,  907, 1782, 1680, 1821, 1762, 1613, 1632, 1777],\n",
       "        [1735, 1839, 1768, 1817, 2523,  911, 1779, 1885, 1816, 1783],\n",
       "        [ 263,  358,  347,  332,  105,  624, 1412,  402,  461, 1475],\n",
       "        [1274, 1282, 1272, 1281, 1374, 1269, 1285, 1270, 1375, 1279],\n",
       "        [1285, 1272, 1274, 1282, 1385, 1281, 1403, 1273, 1375, 1269],\n",
       "        [1368, 1379, 1380, 1367, 1404, 2282, 1372, 1405, 1361, 1373],\n",
       "        [1369, 1367, 1368, 1343, 1405, 1372, 1373, 1386, 1380, 1349],\n",
       "        [1419, 1348, 1345, 1423, 1185, 1420, 1356, 1424, 1349, 1344],\n",
       "        [1492, 1426, 1475, 1827, 1489, 1418, 1431, 2710, 1320, 1417],\n",
       "        [1489, 1304, 1335, 1484, 1466, 1313, 1331, 1438, 1204, 1339],\n",
       "        [1450, 1231, 1238, 1457, 1229, 1233, 1448, 1227, 1228, 1453],\n",
       "        [1206, 1439, 1204, 1438, 1303, 1440, 1205, 1202, 1203, 1463],\n",
       "        [1427, 1417, 1428, 1429, 1188, 1432, 1177, 1309, 1175, 1430],\n",
       "        [1448, 1214, 1444, 1445, 1457, 1238, 1461, 1241, 1222, 1454],\n",
       "        [1451, 1219, 1454, 1232, 1449, 1452, 1448, 1457, 1455, 1238],\n",
       "        [1454, 1232, 1449, 1222, 1451, 1448, 1446, 1220, 1219, 1234],\n",
       "        [1228, 1227, 1229, 1233, 1213, 1452, 1219, 1453, 1226, 1450],\n",
       "        [1244, 1245, 1276, 1278, 1251, 1246, 1271, 1266, 1291, 1248],\n",
       "        [1245, 1244, 1343, 1251, 1386, 1292, 1417, 1291, 1382, 1354],\n",
       "        [1291, 1245, 1251, 1249, 1271, 1277, 1279, 1344, 1265, 1244],\n",
       "        [1284, 1269, 1288, 1268, 1397, 1272, 1265, 1285, 1396, 1287],\n",
       "        [ 665, 1050,  659,  904,  664, 1076,  660, 1069, 1171, 1172],\n",
       "        [1074, 1069,  904, 1076,  659, 1082, 1080, 1077,  665, 1511],\n",
       "        [1081, 1080, 1082, 1077, 1050, 1069, 1511, 1073, 2741, 1064],\n",
       "        [1172, 1171, 2739, 2738, 2744,  664, 2743,  660,  665, 1050],\n",
       "        [ 665, 2738, 1050, 1171,  904,  659, 1066, 2741,  664, 2743],\n",
       "        [1082, 1080, 1069, 1074,  662, 1067, 1081, 1050, 1076,  906],\n",
       "        [2671, 2675, 2683, 2674, 2687, 2679, 1539, 2658, 2654, 2665],\n",
       "        [2675, 2683, 2671, 2665, 2656,  844, 2287, 2685, 2663, 2668]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_embedding_tensor = torch.stack(client_features).squeeze(1)\n",
    "embedding_tensor = torch.stack(reference_features).squeeze(1)\n",
    "\n",
    "cosine_similarities = torch.mm(client_embedding_tensor, embedding_tensor.t())\n",
    "\n",
    "closest_indices = torch.argsort(cosine_similarities, dim=1,descending=True)[:,:10]\n",
    "closest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = {}\n",
    "\n",
    "for i, img in enumerate(img_list):\n",
    "    file_name = img.split(\"\\\\\")[-1]\n",
    "    img_id = file_name.split(\".\")[0]\n",
    "    guesses[img_id] = [reference_images[ind] for ind in closest_indices[i].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.9090909090909"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = 0\n",
    "nb_guess = 0\n",
    "for answer_key in answers.keys():\n",
    "    found = 0\n",
    "    if answer_key in guesses.keys():\n",
    "        nb_guess += 1\n",
    "        for value_guess in guesses[answer_key]:\n",
    "            for value_answ in answers[answer_key]:\n",
    "                if value_answ in value_guess and not found:\n",
    "                    accuracy += 1\n",
    "                    found = 1\n",
    "\n",
    "accuracy/nb_guess*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
